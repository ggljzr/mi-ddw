{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Text mining\n",
    "\n",
    "In this task we will use `nltk` package to recognize [named entities](https://en.wikipedia.org/wiki/Named-entity_recognition) and classify in a given text (in this case [article](https://en.wikipedia.org/wiki/American_Revolution) about American Revolution from Wikipedia).\n",
    "\n",
    "`nltk.ne_chunk` function can be used for both recognition and classification of named entities. We will aslo implement custom NER function to recognize entities, and custom function to classify named entities using their Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Suppress `wikipedia` package warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Helper functions to process output of `nltk.ne_chunk` and to count frequency of named entities in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def count_entites(entity, text):\n",
    "    s = entity\n",
    "    \n",
    "    if type(entity) is tuple:\n",
    "        s = entity[0]\n",
    "    \n",
    "    return len(re.findall(s, text))\n",
    "\n",
    "def get_top_n(entities, text, n):\n",
    "    a = [ (e, count_entites(e, text)) for e in entities]\n",
    "    a.sort(key=lambda x: x[1], reverse=True)\n",
    "    return a[0:n]\n",
    "\n",
    "# For a list of entities found by nltk.ne_chunks:\n",
    "# returns (entity, label) if it is a single word or\n",
    "# concatenates multiple word named entities into single string\n",
    "def get_entity(entity):\n",
    "    if isinstance(entity, tuple) and entity[1][:2] == 'NE':\n",
    "        return entity\n",
    "    if isinstance(entity, nltk.tree.Tree):\n",
    "        text = ' '.join([word for word, tag in entity.leaves()])\n",
    "        return (text, entity.label())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since `nltk.ne_chunks` tends to put same named entities into more classes (like 'American' : 'ORGANIZATION' and 'American' : 'GPE'), we would want to filter these duplicities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# returns list of named entities in a form [(entity_text, entity_label), ...]\n",
    "def extract_entities(chunk):\n",
    "    data = []\n",
    "\n",
    "    for entity in chunk:\n",
    "        d = get_entity(entity)\n",
    "        if d is not None and d[0] not in [e[0] for e in data]:\n",
    "            data.append(d)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our custom NER functio from example [here](http://www.nltk.org/book/ch07.html#tab-ne-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def custom_NER(tagged):\n",
    "    entities = []\n",
    "    \n",
    "    entity = []\n",
    "    for word in tagged:\n",
    "        if word[1][:2] == 'NN' or (entity and word[1][:2] == 'IN'):\n",
    "            entity.append(word)\n",
    "        else:\n",
    "            if entity and entity[-1][1].startswith('IN'):\n",
    "                entity.pop()\n",
    "            if entity:\n",
    "                s = ' '.join(e[0] for e in entity)\n",
    "                if s not in entities and s[0].isupper() and len(s) > 1:\n",
    "                    entities.append(s)\n",
    "            entity = []\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Loading processed article, approximately 500 sentences. Regex substitution removes reference links (e.g. [12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text = None\n",
    "with open('text', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = re.sub(r'\\[[0-9]*\\]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we try to recognize entities with both `nltk.ne_chunk` and our `custom_NER` function and print 10 most frequent entities.\n",
    "\n",
    "Yielded results seem to be fairly similar. `nltk.ne_chunk` function also added basic classification [tags](http://www.nltk.org/book/ch07.html#tab-ne-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne_chunked:\n",
      "('British', 'GPE') count: 154\n",
      "('America', 'GPE') count: 145\n",
      "('American', 'GPE') count: 130\n",
      "('New', 'ORGANIZATION') count: 51\n",
      "('Loyalist', 'GPE') count: 46\n",
      "('Americans', 'GPE') count: 44\n",
      "('Britain', 'GPE') count: 40\n",
      "('Patriot', 'GPE') count: 38\n",
      "('Revolution', 'ORGANIZATION') count: 38\n",
      "('Loyalists', 'ORGANIZATION') count: 37\n",
      "('Congress', 'ORGANIZATION') count: 35\n",
      "('Patriots', 'GPE') count: 29\n",
      "('Boston', 'GPE') count: 29\n",
      "('New York', 'GPE') count: 29\n",
      "('American Revolution', 'ORGANIZATION') count: 24\n",
      "('Parliament', 'ORGANIZATION') count: 24\n",
      "('United States', 'GPE') count: 20\n",
      "('French', 'GPE') count: 19\n",
      "('Washington', 'GPE') count: 19\n",
      "('Continental', 'ORGANIZATION') count: 18\n",
      "\n",
      "custom NER:\n",
      "British count: 154\n",
      "America count: 145\n",
      "Loyalist count: 46\n",
      "Americans count: 44\n",
      "Britain count: 40\n",
      "Revolution count: 38\n",
      "Patriot count: 38\n",
      "Loyalists count: 37\n",
      "Congress count: 35\n",
      "Patriots count: 29\n",
      "Boston count: 29\n",
      "New York count: 29\n",
      "Parliament count: 24\n",
      "American Revolution count: 24\n",
      "United States count: 20\n",
      "Washington count: 19\n",
      "French count: 19\n",
      "War count: 18\n",
      "Act count: 18\n",
      "King count: 17\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "ne_chunked = nltk.ne_chunk(tagged, binary=False)\n",
    "ex = extract_entities(ne_chunked)\n",
    "ex_custom = custom_NER(tagged)\n",
    "\n",
    "top_ex = get_top_n(ex, text, 20)\n",
    "top_ex_custom = get_top_n(ex_custom, text, 20)\n",
    "print('ne_chunked:')\n",
    "for e in top_ex:\n",
    "    print('{} count: {}'.format(e[0], e[1]))\n",
    "print()\n",
    "print('custom NER:')\n",
    "for e in top_ex_custom:\n",
    "    print('{} count: {}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we would want to do our own classification, using Wikipedia articles for each named entity. Idea is to find article matching entity string (for example 'America') and then create a noun phrase from its first sentence. When no suitable article or description is found, entity classification will be 'Thing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_noun_phrase(entity, sentence):\n",
    "    t = nltk.pos_tag([word for word in nltk.word_tokenize(sentence)])\n",
    "    phrase = []\n",
    "    stage = 0\n",
    "    for word in t:\n",
    "        if word[0] in ('is', 'was', 'were', 'are', 'refers') and stage == 0:\n",
    "            stage = 1\n",
    "            continue\n",
    "        elif stage == 1:\n",
    "            if word[1] in ('NN', 'JJ', 'VBD', 'CD', 'NNP', 'NNPS', 'RBS', 'IN', 'NNS'):\n",
    "                phrase.append(word)\n",
    "            elif word[1] in ('DT', ',', 'CC', 'TO', 'POS'):\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    if len(phrase) > 1 and phrase[-1][1] == 'IN':\n",
    "        phrase.pop()\n",
    "        \n",
    "    phrase = ' '.join([ word[0] for word in phrase ])\n",
    "    \n",
    "    if phrase == '':\n",
    "        phrase = 'Thing'\n",
    "        \n",
    "    return {entity : phrase}\n",
    "\n",
    "def get_wiki_desc(entity, wiki='en'):\n",
    "    wikipedia.set_lang(wiki)\n",
    "    \n",
    "    try:\n",
    "        fs = wikipedia.summary(entity, sentences=1)\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        fs = wikipedia.summary(e.options[0], sentences=1)\n",
    "    except wikipedia.PageError:\n",
    "        return {entity : 'Thing'}\n",
    "    \n",
    "    #fs = nltk.sent_tokenize(page.summary)[0]\n",
    "    return get_noun_phrase(entity, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obivously this classification is way more specific than tags used by `nltk.ne_chunk`. We can also see that both NER methods mistook common words for entities unrelated to the article (for example 'New'). \n",
    "\n",
    "Since `custom_NER` function relies on uppercase letters to recognize entities, this can be commonly caused by first words in sentences.\n",
    "\n",
    "The lack of description for entity 'America' is caused by simple way `get_noun_phrase` function constructs description. It looks for basic words like 'is', so more advanced language can throw it off. This could be fixed by searching [simple english](https://simple.wikipedia.org/wiki/Main_Page) Wikipedia or using it as a fallback when no suitable phrase is found on normal english Wikipedia (for example compare article about Americas on [simple](https://simple.wikipedia.org/wiki/Americas) and [normal](https://en.wikipedia.org/wiki/Americas) wiki).\n",
    "\n",
    "I also tried to search for more general verb (presen tense verb, tag 'VBZ'), but this yielded worse results. Other improvement could be simply expanding the verb list in `get_noun_phrase` with other suitable verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'British': 'sovereign country in western Europe'}\n",
      "{'America': 'Thing'}\n",
      "{'American': 'constitutional federal republic'}\n",
      "{'New': 'South Korean single-place paraglider'}\n",
      "{'Loyalist': 'individual allegiance toward established government political party sovereign'}\n",
      "{'Americans': 'citizens of United States of America'}\n",
      "{'Britain': 'sovereign country in western Europe'}\n",
      "{'Patriot': 'conservative talk radio channel on Sirius Satellite Radio channel 125 XM Satellite Radio channel 125 [ 1 ]'}\n",
      "{'Revolution': 'fundamental change in political power organizational structures'}\n",
      "{'Loyalists': 'individual allegiance toward established government political party sovereign'}\n",
      "{'Congress': 'formal meeting of representatives of different nations constituent states independent organizations'}\n",
      "{'Patriots': '1994 American film'}\n",
      "{'Boston': 'capital most populous city of Commonwealth of Massachusetts in United States'}\n",
      "{'New York': 'state in northeastern United States'}\n",
      "{'American Revolution': 'political upheaval'}\n",
      "{'Parliament': 'legislative elected body of government'}\n",
      "{'United States': 'constitutional federal republic'}\n",
      "{'French': 'country with territory in western Europe several overseas regions territories'}\n",
      "{'Washington': 'American politician soldier'}\n",
      "{'Continental': 'one of several'}\n"
     ]
    }
   ],
   "source": [
    "for entity in top_ex:\n",
    "    print(get_wiki_desc(entity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'British': 'sovereign country in western Europe'}\n",
      "{'America': 'Thing'}\n",
      "{'Loyalist': 'individual allegiance toward established government political party sovereign'}\n",
      "{'Americans': 'citizens of United States of America'}\n",
      "{'Britain': 'sovereign country in western Europe'}\n",
      "{'Revolution': 'fundamental change in political power organizational structures'}\n",
      "{'Patriot': 'conservative talk radio channel on Sirius Satellite Radio channel 125 XM Satellite Radio channel 125 [ 1 ]'}\n",
      "{'Loyalists': 'individual allegiance toward established government political party sovereign'}\n",
      "{'Congress': 'formal meeting of representatives of different nations constituent states independent organizations'}\n",
      "{'Patriots': '1994 American film'}\n",
      "{'Boston': 'capital most populous city of Commonwealth of Massachusetts in United States'}\n",
      "{'New York': 'state in northeastern United States'}\n",
      "{'Parliament': 'legislative elected body of government'}\n",
      "{'American Revolution': 'political upheaval'}\n",
      "{'United States': 'constitutional federal republic'}\n",
      "{'Washington': 'American politician soldier'}\n",
      "{'French': 'country with territory in western Europe several overseas regions territories'}\n",
      "{'War': 'state of armed conflict between societies'}\n",
      "{'Act': 'activity'}\n",
      "{'King': 'electoral district in Australian state of New South Wales'}\n"
     ]
    }
   ],
   "source": [
    "for entity in top_ex_custom:\n",
    "    print(get_wiki_desc(entity[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When searching simple wiki, entity 'Americas' gets fairly reasonable description. However there seems to be an issue with handling `DisambiguationError` in some cases when looking for first page in `DisambiguationError.options` raises another `DisambiguationError` (even if pages from `.options` should be guaranteed hit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Americas': 'landmass'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wiki_desc('Americas', wiki='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
