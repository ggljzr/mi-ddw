{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining\n",
    "\n",
    "In this task we will use `nltk` package to recognize [named entities](https://en.wikipedia.org/wiki/Named-entity_recognition) and classify in a given text (in this case [article](https://en.wikipedia.org/wiki/American_Revolution) about American Revolution from Wikipedia).\n",
    "\n",
    "`nltk.ne_chunk` function can be used for both recognition and classification of named entities. We will aslo implement custom NER function to recognize entities, and custom function to classify named entities using their Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress `wikipedia` package warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to process output of `nltk.ne_chunk` and to count frequency of named entities in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_entites(entity, text):\n",
    "    s = entity\n",
    "    \n",
    "    if type(entity) is tuple:\n",
    "        s = entity[0]\n",
    "    \n",
    "    return len(re.findall(s, text))\n",
    "\n",
    "def get_top_n(entities, text, n):\n",
    "    a = [ (e, count_entites(e, text)) for e in entities]\n",
    "    a.sort(key=lambda x: x[1], reverse=True)\n",
    "    return a[0:n]\n",
    "\n",
    "# For a list of entities found by nltk.ne_chunks:\n",
    "# returns (entity, label) if it is a single word or\n",
    "# concatenates multiple word named entities into single string\n",
    "def get_entity(entity):\n",
    "    if isinstance(entity, tuple) and entity[1][:2] == 'NE':\n",
    "        return entity\n",
    "    if isinstance(entity, nltk.tree.Tree):\n",
    "        text = ' '.join([word for word, tag in entity.leaves()])\n",
    "        return (text, entity.label())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `nltk.ne_chunks` tends to put same named entities into more classes (like 'American' : 'ORGANIZATION' and 'American' : 'GPE'), we would want to filter these duplicities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# returns list of named entities in a form [(entity_text, entity_label), ...]\n",
    "def extract_entities(chunk):\n",
    "    data = []\n",
    "\n",
    "    for entity in chunk:\n",
    "        d = get_entity(entity)\n",
    "        if d is not None and d[0] not in [e[0] for e in data]:\n",
    "            data.append(d)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom NER function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def custom_NER(tagged):\n",
    "    entities = []\n",
    "    \n",
    "    entity = []\n",
    "    for word in tagged:\n",
    "        if word[1][:2] == 'NN' or (entity and word[1][:2] == 'IN'):\n",
    "            entity.append(word)\n",
    "        else:\n",
    "            if entity and entity[-1][1].startswith('IN'):\n",
    "                entity.pop()\n",
    "            if entity:\n",
    "                s = ' '.join(e[0] for e in entity)\n",
    "                if s not in entities and s[0].isupper():\n",
    "                    entities.append(s)\n",
    "            entity = []\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading processed article, approximately 500 sentences. Regex substitution removes reference links (e.g. [12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text = None\n",
    "with open('text', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = re.sub(r'\\[[0-9]*\\]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to recognize entities with both `nltk.ne_chunk` and our `custom_NER` function and print 10 most frequent entities.\n",
    "\n",
    "Yielded results seem to be fairly similar. `nltk.ne_chunk` function also added basic classification [tags](http://www.nltk.org/book/ch07.html#tab-ne-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne_chunked:\n",
      "('British', 'GPE') count: 154\n",
      "('America', 'GPE') count: 145\n",
      "('American', 'GPE') count: 130\n",
      "('New', 'ORGANIZATION') count: 51\n",
      "('Loyalist', 'GPE') count: 46\n",
      "('Americans', 'GPE') count: 44\n",
      "('Britain', 'GPE') count: 40\n",
      "('Patriot', 'GPE') count: 38\n",
      "('Revolution', 'ORGANIZATION') count: 38\n",
      "('Loyalists', 'ORGANIZATION') count: 37\n",
      "\n",
      "custom NER:\n",
      "A count: 277\n",
      "British count: 154\n",
      "America count: 145\n",
      "Loyalist count: 46\n",
      "Americans count: 44\n",
      "Britain count: 40\n",
      "Revolution count: 38\n",
      "Patriot count: 38\n",
      "Loyalists count: 37\n",
      "Congress count: 35\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "ne_chunked = nltk.ne_chunk(tagged, binary=False)\n",
    "ex = extract_entities(ne_chunked)\n",
    "ex_custom = custom_NER(tagged)\n",
    "\n",
    "top_ex = get_top_n(ex, text, 10)\n",
    "top_ex_custom = get_top_n(ex_custom, text, 10)\n",
    "print('ne_chunked:')\n",
    "for e in top_ex:\n",
    "    print('{} count: {}'.format(e[0], e[1]))\n",
    "print()\n",
    "print('custom NER:')\n",
    "for e in top_ex_custom:\n",
    "    print('{} count: {}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_noun_phrase(entity, sentence):\n",
    "    t = nltk.pos_tag([word for word in nltk.word_tokenize(sentence)])\n",
    "    phrase = ''\n",
    "    stage = 0\n",
    "    for word in t:\n",
    "        if word[0] in ('is', 'was', 'were', 'are') and stage == 0:\n",
    "            stage = 1\n",
    "            continue\n",
    "        elif stage == 1:\n",
    "            if word[1] in ('NN', 'JJ', 'VBD', 'CD', 'NNP', 'NNPS', 'RBS'):\n",
    "                phrase = phrase + ' ' + word[0]\n",
    "            elif word[1] in ('DT', ',', 'CC', 'IN'):\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "                \n",
    "    return {entity : phrase[1:]}\n",
    "\n",
    "def get_wiki_desc(entity):\n",
    "    try:\n",
    "        page = wikipedia.page(entity)\n",
    "    except wikipedia.DisambiguationError:\n",
    "        return {entity : 'Thing'}\n",
    "    \n",
    "    fs = nltk.sent_tokenize(page.summary)[0]\n",
    "    return get_noun_phrase(entity, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'British': 'Thing'}\n",
      "{'America': 'Thing'}\n",
      "{'American': 'Thing'}\n",
      "{'American': 'Thing'}\n",
      "{'New': 'Thing'}\n",
      "{'Loyalist': ''}\n",
      "{'Loyalist': ''}\n",
      "{'Americans': ''}\n",
      "{'Americans': ''}\n",
      "{'Britain': 'Thing'}\n"
     ]
    }
   ],
   "source": [
    "for entity in top_ex:\n",
    "    print(get_wiki_desc(entity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 'first letter first vowel ISO basic Latin alphabet'}\n",
      "{'British': 'Thing'}\n",
      "{'America': 'Thing'}\n",
      "{'Loyalist': ''}\n",
      "{'Americans': ''}\n",
      "{'Britain': 'Thing'}\n",
      "{'Revolution': 'fundamental change political power organizational'}\n",
      "{'Patriot': 'Thing'}\n",
      "{'Loyalists': ''}\n",
      "{'Congress': 'formal meeting'}\n"
     ]
    }
   ],
   "source": [
    "for entity in top_ex_custom:\n",
    "    print(get_wiki_desc(entity[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
